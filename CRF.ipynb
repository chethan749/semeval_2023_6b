{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task B: Named Entity Recognition with CRF on Hindi Dataset. (Total: 60 Points out of 100)\n",
    "\n",
    "In this part, you will use a CRF to implement a named entity recognition tagger.\n",
    "We have implemented a CRF for you in crf.py along with some functions to build, and pad feature vectors. Your job is to add more features to learn a better tagger. Then you need to complete the traiing loop implementation.\n",
    "\n",
    "Finally, you can checkout the code in `crf.py` -- reflect on CRFs and span tagging, and answer the discussion questions.\n",
    "\n",
    "\n",
    "We will use the Hindi NER dataset at: https://github.com/cfiltnlp/HiNER\n",
    "\n",
    "The first step would be to download the repo into your current folder of the Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'HiNER' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "# !git clone https://github.com/cfiltnlp/HiNER.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is so that you don't have to restart the kernel everytime you edit hmm.py\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First we load the data and labels. Feel free to explore them below.\n",
    "\n",
    "Since we have provided a seperate train and dev split, there is not need to split the data yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train sample ['Before', 'The', 'Madurai', 'Bench', 'Of', 'Madras', 'High', 'Court', 'Dated', ':', '23/12/2011', 'Coram', 'The', 'Honourable', 'Mr.', 'Justice', 'V.Ramasubramanian', 'Civil', 'Revision', 'Petition', '(', 'Npd)(Md', ')', 'No.1123', 'of', '2006', 'And', 'M.P.No.2', 'of', '2006', '1', '.', 'Ayisha', 'Beevi', '2', '.', 'Beevija', '3', '.', 'Hadijath', 'Beevi', '4', '.', 'Yunusa', 'Begam', '5', '.', 'Syed', 'Ali', '6', '.', 'Sumaya', 'Begam', '7', '.', 'Mohamed', 'Yoosuf', '8', '.', 'Mohamed', 'Ismail', '9', '.', 'Razira', 'Beevi', '10.Shabi', 'Mohamed', '11.Zakir', 'Mugain', '12.Ferosh', 'Khan', '13.Augustin', '14.Dr', '.', 'T.C.Joseph', '.....', 'Petitioners', 'Vs', '.', '1', '.', 'Sheik', 'Mydeen', '2', '.', 'A.P.Nelson', '3', '.', 'Chandrakala', 'Ruben', '.....', 'Respondents', '-----', 'Petition', 'under', 'Article', '227', 'of', ' ', 'the', 'Constitution', 'of', 'India', 'against', 'the', 'fair', 'and', 'decretal', 'order', 'dated', '18.10.2006', 'made', 'in', 'E.P.No.201', 'of', '2000', 'in', 'O.S.No.420', 'of', '1976', 'passed', 'by', 'the', 'Principal', 'District', 'Munsif', ',', 'Padmanabhapuram', '.', '!', 'For', 'Petitioner', '...', '  ', 'Mr.', 'K.N.Thampi', '^For', 'Respondent-1', '...', '  ', 'Mr.', 'Prabhu', 'Rajadurai', 'For', 'Respondents', '2&3', '...', '  ', 'Mr.', 'A.Arumugham', '-----', ':', 'Order'] ['O', 'O', 'B-COURT', 'I-COURT', 'I-COURT', 'I-COURT', 'I-COURT', 'I-COURT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-JUDGE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PETITIONER', 'I-PETITIONER', 'O', 'O', 'B-PETITIONER', 'O', 'O', 'B-PETITIONER', 'I-PETITIONER', 'O', 'O', 'B-PETITIONER', 'I-PETITIONER', 'O', 'O', 'B-PETITIONER', 'I-PETITIONER', 'O', 'O', 'B-PETITIONER', 'I-PETITIONER', 'O', 'O', 'B-PETITIONER', 'I-PETITIONER', 'O', 'O', 'B-PETITIONER', 'I-PETITIONER', 'O', 'O', 'B-PETITIONER', 'I-PETITIONER', 'B-PETITIONER', 'I-PETITIONER', 'B-PETITIONER', 'I-PETITIONER', 'B-PETITIONER', 'I-PETITIONER', 'B-PETITIONER', 'B-PETITIONER', 'I-PETITIONER', 'I-PETITIONER', 'O', 'O', 'O', 'O', 'O', 'O', 'B-RESPONDENT', 'I-RESPONDENT', 'O', 'O', 'B-RESPONDENT', 'O', 'O', 'B-RESPONDENT', 'I-RESPONDENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LAWYER', 'O', 'O', 'O', 'O', 'O', 'B-LAWYER', 'I-LAWYER', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LAWYER', 'O', 'O', 'O']\n",
      "\n",
      "labels2i {'<PAD>': 0, 'I-STATUTE': 1, 'B-GPE': 2, 'B-WITNESS': 3, 'B-ORG': 4, 'I-PETITIONER': 5, 'I-DATE': 6, 'I-ORG': 7, 'B-STATUTE': 8, 'I-JUDGE': 9, 'B-RESPONDENT': 10, 'I-CASE_NUMBER': 11, 'I-COURT': 12, 'B-JUDGE': 13, 'I-PROVISION': 14, 'I-RESPONDENT': 15, 'B-OTHER_PERSON': 16, 'B-LAWYER': 17, 'O': 18, 'I-PRECEDENT': 19, 'B-CASE_NUMBER': 20, 'B-PETITIONER': 21, 'I-GPE': 22, 'I-LAWYER': 23, 'B-DATE': 24, 'B-COURT': 25, 'B-PRECEDENT': 26, 'B-PROVISION': 27, 'I-OTHER_PERSON': 28, 'I-WITNESS': 29}\n"
     ]
    }
   ],
   "source": [
    "from crf import load_data, make_labels2i\n",
    "\n",
    "train_filepath = \"preamble_train.txt\"\n",
    "# dev_filepath = \"./HiNER/data/collapsed/validation.conll\"\n",
    "labels_filepath = \"all_labels.txt\"\n",
    "\n",
    "train_sents, train_tag_sents = load_data(train_filepath)\n",
    "dev_sents, dev_tag_sents = train_sents[int(len(train_sents) * 9 // 10):], train_tag_sents[int(len(train_sents) * 9 // 10):]\n",
    "train_sents, train_tag_sents = train_sents[:int(len(train_sents) * 9 // 10)], train_tag_sents[:int(len(train_sents) * 9 // 10)]\n",
    "labels2i = make_labels2i(labels_filepath)\n",
    "\n",
    "print(\"train sample\", train_sents[2], train_tag_sents[2])\n",
    "print()\n",
    "print(\"labels2i\", labels2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1561"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sents) + len(dev_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering. (Total 30 points)\n",
    "\n",
    "Notice that we are **learning** features to some extent: we start with one unique feature for every possible word. You can refer to figure 8.15 in the textbook for some good baseline features to try.\n",
    "![image.png](image2.png)\n",
    "\n",
    "There is no need to worry about embeddings now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hindi POS Tagger   (10 Points)\n",
    "\n",
    "Although this step is not entirely necessary, if you want to use the HMM pos tagger to extract feature corresponding to the pos of the word in the sentence, we need to add this into the pipeline.\n",
    "\n",
    "You get 10 points if you use your pos_tagger to featurize the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id of the <unk> token: 2186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package indian to /home/preetham/nltk_data...\n",
      "[nltk_data]   Package indian is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from hmm import get_hindi_dataset\n",
    "import pickle\n",
    "from typing import List\n",
    "\n",
    "words, tags, observation_dict, state_dict, all_observation_ids, all_state_ids = get_hindi_dataset()\n",
    "\n",
    "# we need to add the id for unknown word (<unk>) in our observations vocab\n",
    "UNK_TOKEN = '<unk>'\n",
    "\n",
    "observation_dict[UNK_TOKEN] = len(observation_dict)\n",
    "print(\"id of the <unk> token:\", observation_dict[UNK_TOKEN])\n",
    "\n",
    "## load the pos tagger \n",
    "pos_tagger = pickle.load(open('hindi_pos_tagger.pkl', 'rb'))\n",
    "\n",
    "def encode(sentences: List[List[str]]) -> List[List[int]]:\n",
    "    \"\"\"\n",
    "    Using the observation_dict, convert the tokens to ids\n",
    "    unknown words take the id for UNK_TOKEN\n",
    "    \"\"\"\n",
    "    return [\n",
    "        [observation_dict[t] if t in observation_dict else observation_dict[UNK_TOKEN]\n",
    "            for t in sentence]\n",
    "        for sentence in sentences]\n",
    "\n",
    "def get_pos(pos_tagger, sentences) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    The the pos tag for input sentences\n",
    "    \"\"\"\n",
    "    sentence_ids = encode(sentences)\n",
    "    decoded_pos_ids = pos_tagger.decode(sentence_ids)\n",
    "    return [\n",
    "        [tags[i] for i in d_ids]\n",
    "        for d_ids in decoded_pos_ids\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering Functions (20 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_tags(json_path):\n",
    "    with open(json_path, 'r+') as f:\n",
    "        json_data = json.load(f)\n",
    "\n",
    "    sentences_tags = []\n",
    "    for datapoint in tqdm(json_data):\n",
    "        label_dicts = datapoint['annotations'][0]['result']\n",
    "        sent = datapoint['data']['text']\n",
    "        sent = sent.replace('\\n', '\\t')\n",
    "        doc = nlp(sent)\n",
    "\n",
    "        tag_list = []\n",
    "        for label_dict in label_dicts:\n",
    "            tag_list.append((label_dict['value']['start'], label_dict['value']['end'], label_dict['value']['labels'][0]))\n",
    "\n",
    "        tokens = []\n",
    "        tags = []\n",
    "        pos = []\n",
    "        for token in doc:\n",
    "            tokens.append(token.text)\n",
    "            if len(tag_list) == 0:\n",
    "                tags.append('O')\n",
    "                continue\n",
    "\n",
    "            if token.idx > tag_list[0][1]:\n",
    "                tag_list.pop(0)\n",
    "\n",
    "            if len(tag_list) == 0:\n",
    "                tags.append('O')\n",
    "                continue\n",
    "\n",
    "            if token.idx >= tag_list[0][0] and token.idx < tag_list[0][1]:\n",
    "                if token.idx == tag_list[0][0]:\n",
    "                    tags.append(f'B-{tag_list[0][2]}')\n",
    "                else:\n",
    "                    tags.append(f'I-{tag_list[0][2]}')\n",
    "            else:\n",
    "                tags.append('O')\n",
    "            pos.append(token.pos_)\n",
    "        sentences_tags.append([tokens, tags, pos])\n",
    "    \n",
    "    return sentences_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent_tag_pos = get_sentences_tags(\"NER_TRAIN/NER_TRAIN_JUDGEMENT.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open(\"JUDGEMENT_BIO_POS.txt\", \"w\")\n",
    "# for sent, tag, pos in sent_tag_pos:\n",
    "#     for s, t, p in zip(sent, tag, pos):\n",
    "#         f.write(f\"{s}\\t{t}\\t{p}\")\n",
    "#         f.write(\"\\n\")\n",
    "    \n",
    "#     f.write(\"\\n\")\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_data(sents, tags):\n",
    "#     new_sents = []\n",
    "#     new_tags = []\n",
    "#     for sent, tag in zip(sents, tags):\n",
    "#         tag_index = ['O']  * len(''.join(sent))\n",
    "\n",
    "#         _sum = 0\n",
    "#         for word, tag in zip(sent, tags):\n",
    "#             for i in range(_sum, _sum + len(word)):\n",
    "# #                 print(i, len(tag_index))\n",
    "#                 tag_index[i] = tag\n",
    "#             _sum += len(word)\n",
    "\n",
    "#         spacy_tokens = []\n",
    "#         spacy_tags = []\n",
    "#         _sum = 0\n",
    "#         for token in doc:\n",
    "#             print(len(tag_index), _sum)\n",
    "#             spacy_tokens.append(token.text)\n",
    "#             spacy_tags.append(tag_index[_sum])\n",
    "#             _sum += len(token.text)\n",
    "    \n",
    "#         new_sents.append(spacy_tokens)\n",
    "#         new_tags.append(spacy_tags)\n",
    "    \n",
    "#     return new_sents, new_tags\n",
    "\n",
    "# new_train_sents, new_train_tags = process_data(train_sents, train_tag_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import spacy\n",
    "\n",
    "# TODO: Update this function to add more features\n",
    "#      You can check crf.py for how they are encoded, if interested.\n",
    "def make_features(text: List[str]) -> List[List[int]]:\n",
    "    \"\"\"Turn a text into a feature vector.\n",
    "\n",
    "    Args:\n",
    "        text (List[str]): List of tokens.\n",
    "\n",
    "    Returns:\n",
    "        List[List[int]]: List of feature Lists.\n",
    "    \"\"\"\n",
    "#     sent_tags = get_pos(pos_tagger, [text])[0]\n",
    "    feature_lists = []\n",
    "    # nlp = spacy.load(\"en_core_web_sm\")\n",
    "    # doc = nlp(' '.join(text))\n",
    "    # doc = spacy.tokens.Doc(nlp.vocab, words=text)\n",
    "    pos_tags = nltk.pos_tag(text)\n",
    "    for i, token in enumerate(text):\n",
    "        feats = []\n",
    "        # We add a feature for each unigram.\n",
    "        feats.append(f\"word={token}\")\n",
    "        feats.append(f\"prev_word={'<s>' if i == 0 else text[i - 1]}\")\n",
    "        feats.append(f\"next_word={'</s>' if i == len(text) - 1 else text[i + 1]}\")\n",
    "        feats.append(1 if True in [i.isupper() for i in token] else 0)\n",
    "        feats.append(1 if True in [i.isdigit() for i in token] else 0)\n",
    "        feats.append(1 if token in '\\t\\n' else 0)\n",
    "        feats.append(f\"prefix={token[:3] if len(token) >= 3 else token}\")\n",
    "        feats.append(f\"prefix={token[-3:] if len(token) >= 3 else token}\")\n",
    "        feats.append(len(token))\n",
    "        # feats.append(pos_tags[i])\n",
    "        # feats.append(token.is_stop)\n",
    "        # TODO: Add more features here\n",
    "#         feats.append(sent_tags[i - 1] if i != 0 else '<s>')\n",
    "#         feats.append(sent_tags[i])\n",
    "#         feats.append(sent_tags[i + 1] if i != len(text) - 1 else '</s>')\n",
    "        # We append each feature to a List for the token.\n",
    "        feature_lists.append(feats)\n",
    "\n",
    "    return feature_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def featurize(sents: List[List[str]]) -> List[List[List[str]]]:\n",
    "    \"\"\"Turn the sentences into feature Lists.\n",
    "    \n",
    "    Eg.: For an input of 1 sentence:\n",
    "         [[['I','am','a','student','at','CU','Boulder']]]\n",
    "        Return list of features for every token for every sentence like:\n",
    "        [[\n",
    "         ['word=I',  'prev_word=<S>','pos=PRON',...],\n",
    "         ['word=an', 'prev_word=I'  , 'pos=VB' ,...],\n",
    "         [...]\n",
    "        ]]\n",
    "\n",
    "    Args:\n",
    "        sents (List[List[str]]): A List of sentences, which are Lists of tokens.\n",
    "\n",
    "    Returns:\n",
    "        List[List[List[str]]]: A List of sentences, which are Lists of feature Lists\n",
    "    \"\"\"\n",
    "    feats = []\n",
    "    for sent in sents:\n",
    "        # Gets a List of Lists of feature strings\n",
    "        feats.append(make_features(sent))\n",
    "\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finish the training loop.   (10 Points)\n",
    "\n",
    "See the previous homework, and fill in the missing parts of the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crf import f1_score, predict, PAD_SYMBOL, pad_features, pad_labels\n",
    "import random\n",
    "\n",
    "# TODO: Implement the training loop\n",
    "# HINT: Build upon what we gave you for HW2.\n",
    "# See cell below for how we call this training loop.\n",
    "\n",
    "def training_loop(\n",
    "    num_epochs,\n",
    "    batch_size,\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    dev_features,\n",
    "    dev_labels,\n",
    "    optimizer,\n",
    "    model,\n",
    "    labels2i,\n",
    "    pad_feature_idx\n",
    "):\n",
    "    # TODO: Zip the train features and labels\n",
    "    \n",
    "    # TODO: Randomize them, while keeping them paired.\n",
    "    \n",
    "    # TODO: Build batches\n",
    "    samples = list(zip(train_features, train_labels))\n",
    "    random.shuffle(samples)\n",
    "    batches = []\n",
    "    for i in range(0, len(samples), batch_size):\n",
    "        batches.append(samples[i:i+batch_size])\n",
    "    print(\"Training...\")\n",
    "    for i in range(num_epochs):\n",
    "        losses = []\n",
    "        for batch in tqdm(batches):\n",
    "            # Here we get the features and labels, pad them,\n",
    "            # and build a mask so that our model ignores PADs\n",
    "            # We have abstracted the padding from you for simplicity, \n",
    "            # but please reach out if you'd like learn more.\n",
    "            features, labels = zip(*batch)\n",
    "            features = pad_features(features, pad_feature_idx)\n",
    "            features = torch.stack(features)\n",
    "            # Pad the label sequences to all be the same size, so we\n",
    "            # can form a proper matrix.\n",
    "            labels = pad_labels(labels, labels2i[PAD_SYMBOL])\n",
    "            labels = torch.stack(labels)\n",
    "            mask = (labels != labels2i[PAD_SYMBOL])\n",
    "            # TODO: Empty the dynamic computation graph\n",
    "            optimizer.zero_grad()\n",
    "            # TODO: Run the model. Since we use the pytorch-crf model,\n",
    "            # our forward function returns the positive log-likelihood already.\n",
    "            # We want the negative log-likelihood. See crf.py forward method in NERTagger\n",
    "            loss = -1 * model.forward(features, labels, mask)\n",
    "            # TODO: Backpropogate the loss through our model\n",
    "            loss.backward()\n",
    "            # TODO: Update our coefficients in the direction of the gradient.\n",
    "            optimizer.step()\n",
    "            # TODO: Store the losses for logging\n",
    "            losses.append(loss.item())\n",
    "        \n",
    "        # TODO: Log the average Loss for the epoch\n",
    "        print(f\"epoch {i}, loss: {sum(losses)/len(losses)}\")\n",
    "        # TODO: make dev predictions with the `predict()` function\n",
    "        dev_predictions = predict(model, dev_features)\n",
    "        # TODO: Compute F1 score on the dev set and log it.\n",
    "        dev_f1 = f1_score(dev_predictions, dev_labels, labels2i['O'])\n",
    "        print(f'F1: {dev_f1}')\n",
    "        \n",
    "    # Return the trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the training loop   (10 Points)\n",
    "\n",
    "We have provided the code here, but you can try different hyperparameters and test multiple runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building features set!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1404/1404 [00:00<00:00, 9217.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 74582 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from crf import build_features_set\n",
    "from crf import make_features_dict\n",
    "from crf import encode_features, encode_labels\n",
    "from crf import NERTagger\n",
    "\n",
    "# Build the model and featurized data\n",
    "train_features = featurize(train_sents)\n",
    "dev_features = featurize(dev_sents)\n",
    "\n",
    "# Get the full inventory of possible features\n",
    "all_features = build_features_set(train_features)\n",
    "# Hash all features to a unique int.\n",
    "features_dict = make_features_dict(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_train_features = encode_features(train_features, features_dict)\n",
    "encoded_dev_features = encode_features(dev_features, features_dict)\n",
    "encoded_train_labels = encode_labels(train_tag_sents, labels2i)\n",
    "encoded_dev_labels = encode_labels(dev_tag_sents, labels2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feat, label, sents in zip(dev_features, dev_tag_sents, dev_sents):\n",
    "    if len(feat) != len(label):\n",
    "        print(len(feat), len(label), len(sents))\n",
    "        # print(len(sents), len(feat))\n",
    "        print(sents)\n",
    "        print([x[0] for x in feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "354"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_dev_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:41<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss: 218.72774895754728\n",
      "F1: tensor([0.0304])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:41<00:00,  2.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss: 126.92378442937678\n",
      "F1: tensor([0.1180])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:41<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, loss: 101.9946199763905\n",
      "F1: tensor([0.1957])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:41<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, loss: 98.81249141693115\n",
      "F1: tensor([0.2254])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:41<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, loss: 80.73217404972424\n",
      "F1: tensor([0.2481])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:41<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5, loss: 77.75646565177225\n",
      "F1: tensor([0.2676])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:41<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6, loss: 75.45960413325916\n",
      "F1: tensor([0.2852])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:41<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7, loss: 68.74048228697343\n",
      "F1: tensor([0.3148])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:41<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8, loss: 63.95763377709822\n",
      "F1: tensor([0.3285])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:40<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9, loss: 63.72090489214117\n",
      "F1: tensor([0.3453])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:41<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10, loss: 64.24418254332109\n",
      "F1: tensor([0.3575])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:41<00:00,  2.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11, loss: 57.1717294996435\n",
      "F1: tensor([0.3652])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:41<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12, loss: 57.25643636963584\n",
      "F1: tensor([0.3688])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:40<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13, loss: 52.207093412225895\n",
      "F1: tensor([0.3761])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:41<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14, loss: 52.471501718867906\n",
      "F1: tensor([0.3881])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:41<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15, loss: 52.42497275092385\n",
      "F1: tensor([0.3975])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:40<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16, loss: 47.756753639741376\n",
      "F1: tensor([0.3995])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:47<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17, loss: 50.528560638427734\n",
      "F1: tensor([0.4093])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:41<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 18, loss: 46.82965018532493\n",
      "F1: tensor([0.4155])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:46<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19, loss: 44.551722049713135\n",
      "F1: tensor([0.4188])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:49<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20, loss: 45.39455264264887\n",
      "F1: tensor([0.4290])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:50<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 21, loss: 46.268247539346866\n",
      "F1: tensor([0.4317])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:54<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 22, loss: 43.943980108607896\n",
      "F1: tensor([0.4392])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:40<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 23, loss: 41.80939470637929\n",
      "F1: tensor([0.4361])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:33<00:00,  2.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 24, loss: 42.949281670830466\n",
      "F1: tensor([0.4495])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:32<00:00,  2.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 25, loss: 40.725315072319724\n",
      "F1: tensor([0.4557])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:32<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 26, loss: 40.28819348595359\n",
      "F1: tensor([0.4575])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:32<00:00,  2.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 27, loss: 39.36662262136286\n",
      "F1: tensor([0.4621])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:36<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 28, loss: 39.67181606726213\n",
      "F1: tensor([0.4754])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:48<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 29, loss: 38.70173322070729\n",
      "F1: tensor([0.4764])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:47<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30, loss: 37.59095549583435\n",
      "F1: tensor([0.4742])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:47<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 31, loss: 39.11216523430564\n",
      "F1: tensor([0.4847])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:48<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 32, loss: 37.2170372876254\n",
      "F1: tensor([0.4792])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:48<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 33, loss: 36.06025938554244\n",
      "F1: tensor([0.4803])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:48<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 34, loss: 35.57252145897258\n",
      "F1: tensor([0.4860])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:47<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 35, loss: 35.19763018868186\n",
      "F1: tensor([0.4878])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:48<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 36, loss: 36.08889418298548\n",
      "F1: tensor([0.5015])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:48<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 37, loss: 34.50345451181585\n",
      "F1: tensor([0.4958])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:48<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 38, loss: 35.52600227702748\n",
      "F1: tensor([0.5072])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:48<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 39, loss: 34.0625862641768\n",
      "F1: tensor([0.5065])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:47<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 40, loss: 34.56699152426286\n",
      "F1: tensor([0.5133])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:47<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 41, loss: 34.01993170651522\n",
      "F1: tensor([0.5137])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:47<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 42, loss: 33.228586467829615\n",
      "F1: tensor([0.5132])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:47<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 43, loss: 32.92859411239624\n",
      "F1: tensor([0.5156])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:43<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 44, loss: 32.75744750282981\n",
      "F1: tensor([0.5185])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:45<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 45, loss: 32.72173583507538\n",
      "F1: tensor([0.5234])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:44<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 46, loss: 31.641459996050056\n",
      "F1: tensor([0.5187])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:43<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 47, loss: 32.168273557316176\n",
      "F1: tensor([0.5272])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:40<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 48, loss: 31.5435828295621\n",
      "F1: tensor([0.5258])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:44<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 49, loss: 31.330914215608075\n",
      "F1: tensor([0.5262])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:44<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, loss: 30.514008391987193\n",
      "F1: tensor([0.5271])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:46<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 51, loss: 31.319462115114387\n",
      "F1: tensor([0.5350])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:45<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 52, loss: 30.084999377077278\n",
      "F1: tensor([0.5302])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:44<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 53, loss: 30.868902065537192\n",
      "F1: tensor([0.5389])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:44<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 54, loss: 29.72886423631148\n",
      "F1: tensor([0.5335])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:46<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 55, loss: 29.594580390236594\n",
      "F1: tensor([0.5375])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:46<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 56, loss: 29.74232110110196\n",
      "F1: tensor([0.5398])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:42<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 57, loss: 29.241073391654275\n",
      "F1: tensor([0.5410])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:41<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 58, loss: 28.912938150492582\n",
      "F1: tensor([0.5384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:40<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 59, loss: 29.35848966511813\n",
      "F1: tensor([0.5522])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:41<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 60, loss: 28.34050679206848\n",
      "F1: tensor([0.5399])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:41<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 61, loss: 28.098762078718707\n",
      "F1: tensor([0.5414])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:41<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 62, loss: 27.84396422993053\n",
      "F1: tensor([0.5487])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:41<00:00,  2.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 63, loss: 27.789348363876343\n",
      "F1: tensor([0.5477])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:41<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 64, loss: 27.433739380402997\n",
      "F1: tensor([0.5487])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:41<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 65, loss: 27.389133594252847\n",
      "F1: tensor([0.5487])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:41<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 66, loss: 27.39212313565341\n",
      "F1: tensor([0.5531])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:41<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 67, loss: 26.88813369924372\n",
      "F1: tensor([0.5512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:40<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 68, loss: 26.825607657432556\n",
      "F1: tensor([0.5538])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:40<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 69, loss: 26.76928745616566\n",
      "F1: tensor([0.5564])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:42<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 70, loss: 26.417010404846884\n",
      "F1: tensor([0.5557])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:41<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 71, loss: 26.47853363643993\n",
      "F1: tensor([0.5579])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:41<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 72, loss: 26.02996814250946\n",
      "F1: tensor([0.5592])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:40<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 73, loss: 26.06117890097878\n",
      "F1: tensor([0.5584])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:41<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 74, loss: 25.69644710150632\n",
      "F1: tensor([0.5602])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:41<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 75, loss: 26.099553888494317\n",
      "F1: tensor([0.5625])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:42<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 76, loss: 25.45732134038752\n",
      "F1: tensor([0.5580])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:41<00:00,  2.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 77, loss: 25.271623665636238\n",
      "F1: tensor([0.5613])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:41<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 78, loss: 25.18883280320601\n",
      "F1: tensor([0.5627])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:41<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 79, loss: 25.044839934869245\n",
      "F1: tensor([0.5623])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:44<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 80, loss: 24.842044039206073\n",
      "F1: tensor([0.5632])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:32<00:00,  2.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 81, loss: 25.140495603734795\n",
      "F1: tensor([0.5638])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:31<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 82, loss: 24.541514039039612\n",
      "F1: tensor([0.5656])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:31<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 83, loss: 24.414221709424798\n",
      "F1: tensor([0.5668])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:31<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 84, loss: 24.542176940224387\n",
      "F1: tensor([0.5652])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:31<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 85, loss: 24.305120175535027\n",
      "F1: tensor([0.5654])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:31<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 86, loss: 24.091136314652182\n",
      "F1: tensor([0.5667])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:31<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 87, loss: 23.913568919355217\n",
      "F1: tensor([0.5674])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:32<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 88, loss: 23.77523708343506\n",
      "F1: tensor([0.5671])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:32<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 89, loss: 23.646952033042908\n",
      "F1: tensor([0.5674])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:33<00:00,  2.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 90, loss: 23.536829883402046\n",
      "F1: tensor([0.5690])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:32<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 91, loss: 23.409824002872814\n",
      "F1: tensor([0.5701])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:32<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 92, loss: 23.271356376734648\n",
      "F1: tensor([0.5705])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:32<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 93, loss: 23.145489183339205\n",
      "F1: tensor([0.5732])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:33<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 94, loss: 23.064906803044405\n",
      "F1: tensor([0.5743])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:32<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 95, loss: 22.89218281615864\n",
      "F1: tensor([0.5733])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:32<00:00,  2.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 96, loss: 22.888449116186663\n",
      "F1: tensor([0.5748])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:32<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 97, loss: 22.660954854705118\n",
      "F1: tensor([0.5771])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:32<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 98, loss: 22.578356981277466\n",
      "F1: tensor([0.5787])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 88/88 [00:32<00:00,  2.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 99, loss: 22.457206574353304\n",
      "F1: tensor([0.5792])\n"
     ]
    }
   ],
   "source": [
    "# TODO: Play with hyperparameters here.\n",
    "\n",
    "# Initialize the model.\n",
    "model = NERTagger(len(features_dict), len(labels2i))\n",
    "\n",
    "num_epochs = 100\n",
    "batch_size = 16\n",
    "LR=0.009\n",
    "optimizer = torch.optim.SGD(model.parameters(), LR)\n",
    "\n",
    "model = training_loop(\n",
    "    num_epochs,\n",
    "    batch_size,\n",
    "    encoded_train_features,\n",
    "    encoded_train_labels,\n",
    "    encoded_dev_features,\n",
    "    encoded_dev_labels,\n",
    "    optimizer,\n",
    "    model,\n",
    "    labels2i,\n",
    "    features_dict[PAD_SYMBOL]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_precision(\n",
    "    predicted_labels: List[torch.Tensor],\n",
    "    true_labels: List[torch.Tensor],\n",
    "    b_tag_idx: int,\n",
    "    i_tag_idx: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    Precision is True Positives / All Positives Predictions\n",
    "    \"\"\"\n",
    "    TP = torch.tensor([0])\n",
    "    denom = torch.tensor([0])\n",
    "    for pred, true in zip(predicted_labels, true_labels):\n",
    "        TP += sum((pred == true)[pred == b_tag_idx]) + sum((pred == true)[pred == i_tag_idx])\n",
    "        denom += sum(pred == b_tag_idx) + sum(pred == i_tag_idx)\n",
    "\n",
    "    # Avoid division by 0\n",
    "    denom = torch.tensor(1) if denom == 0 else denom\n",
    "    return TP / denom\n",
    "\n",
    "\n",
    "def class_recall(\n",
    "    predicted_labels: List[torch.Tensor],\n",
    "    true_labels: List[torch.Tensor],\n",
    "    b_tag_idx: int,\n",
    "    i_tag_idx: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    Recall is True Positives / All Positive Labels\n",
    "    \"\"\"\n",
    "    TP = torch.tensor([0])\n",
    "    denom = torch.tensor([0])\n",
    "    for pred, true in zip(predicted_labels, true_labels):\n",
    "        TP += sum((pred == true)[true == b_tag_idx]) + sum((pred == true)[true == i_tag_idx])\n",
    "        denom += sum(true == b_tag_idx) + sum(true == i_tag_idx)\n",
    "\n",
    "    # Avoid division by 0\n",
    "    denom = torch.tensor(1) if denom == 0 else denom\n",
    "    return TP / denom\n",
    "\n",
    "\n",
    "def class_f1_score(predicted_labels, true_labels, b_tag_idx, i_tag_idx):\n",
    "    \"\"\"\n",
    "    F1 score is the harmonic mean of precision and recall\n",
    "    \"\"\"\n",
    "    P = class_precision(predicted_labels, true_labels, b_tag_idx, i_tag_idx)\n",
    "    R = class_recall(predicted_labels, true_labels, b_tag_idx, i_tag_idx)\n",
    "    return 2*P*R/(P+R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_predictions = predict(model, encoded_dev_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_precision(dev_predictions, encoded_dev_labels, labels2i['B-OTHER_PERSON'], labels2i['I-OTHER_PERSON'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COURT: tensor([0.7109])\n",
      "PETITIONER: tensor([0.4163])\n",
      "RESPONDENT: tensor([0.5166])\n",
      "JUDGE: tensor([0.6693])\n",
      "LAWYER: tensor([0.7289])\n"
     ]
    }
   ],
   "source": [
    "check_tags = [\"COURT\", \"PETITIONER\", \"RESPONDENT\", \"JUDGE\", \"LAWYER\"]\n",
    "f1_scores = []\n",
    "for tag in check_tags:\n",
    "    f1 = class_f1_score(dev_predictions, encoded_dev_labels, labels2i[f'B-{tag}'], labels2i[f'I-{tag}'])\n",
    "    print(f\"{tag}: {f1}\")\n",
    "    f1_scores.append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6084])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(f1_scores) / len(f1_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
